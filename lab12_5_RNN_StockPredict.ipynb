{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(2017)\n",
    "\n",
    "#\"DISPLAY\" in os.environ\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    matplotlib.use(\"Agg\")\n",
    "    \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmaxScaler(data):\n",
    "    num = data-np.min(data,0)\n",
    "    denom = np.max(data,0)-np.min(data,0)\n",
    "    return num/(denom+1e-7)\n",
    "\n",
    "seq_len = 7\n",
    "data_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.001\n",
    "iter = 500\n",
    "\n",
    "xy = np.loadtxt('data-02-stock_daily.csv', delimiter=',')\n",
    "#print(xy)\n",
    "xy = minmaxScaler(xy[::-1])\n",
    "x = xy\n",
    "y = xy[:,[-1]]\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(y)-seq_len):\n",
    "    _x = x[i:i+seq_len]\n",
    "    _y = y[i+seq_len]\n",
    "    #print(_x,\"->\", _y)\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0 loss:94.5844955444336\n",
      "rmse : 0.8375473022460938\n",
      "step:1 loss:88.1770248413086\n",
      "rmse : 0.8129920363426208\n",
      "step:2 loss:82.07797241210938\n",
      "rmse : 0.7886771559715271\n",
      "step:3 loss:76.2770767211914\n",
      "rmse : 0.76460200548172\n",
      "step:4 loss:70.76498413085938\n",
      "rmse : 0.7407844066619873\n",
      "step:5 loss:65.53424072265625\n",
      "rmse : 0.7172409296035767\n",
      "step:6 loss:60.57829666137695\n",
      "rmse : 0.6939812302589417\n",
      "step:7 loss:55.890445709228516\n",
      "rmse : 0.6710087656974792\n",
      "step:8 loss:51.463592529296875\n",
      "rmse : 0.6483228802680969\n",
      "step:9 loss:47.29019546508789\n",
      "rmse : 0.6259202361106873\n",
      "step:10 loss:43.36244583129883\n",
      "rmse : 0.6037969589233398\n",
      "step:11 loss:39.67255401611328\n",
      "rmse : 0.5819499492645264\n",
      "step:12 loss:36.212921142578125\n",
      "rmse : 0.5603772401809692\n",
      "step:13 loss:32.97627639770508\n",
      "rmse : 0.5390793085098267\n",
      "step:14 loss:29.95574378967285\n",
      "rmse : 0.5180584788322449\n",
      "step:15 loss:27.14484405517578\n",
      "rmse : 0.4973198175430298\n",
      "step:16 loss:24.53742790222168\n",
      "rmse : 0.4768708050251007\n",
      "step:17 loss:22.127599716186523\n",
      "rmse : 0.45672136545181274\n",
      "step:18 loss:19.909603118896484\n",
      "rmse : 0.43688419461250305\n",
      "step:19 loss:17.877717971801758\n",
      "rmse : 0.4173746109008789\n",
      "step:20 loss:16.026153564453125\n",
      "rmse : 0.39821046590805054\n",
      "step:21 loss:14.34892749786377\n",
      "rmse : 0.37941259145736694\n",
      "step:22 loss:12.839804649353027\n",
      "rmse : 0.3610042929649353\n",
      "step:23 loss:11.492170333862305\n",
      "rmse : 0.34301164746284485\n",
      "step:24 loss:10.298971176147461\n",
      "rmse : 0.325463205575943\n",
      "step:25 loss:9.252653121948242\n",
      "rmse : 0.30838990211486816\n",
      "step:26 loss:8.345114707946777\n",
      "rmse : 0.29182472825050354\n",
      "step:27 loss:7.567678451538086\n",
      "rmse : 0.2758026421070099\n",
      "step:28 loss:6.9110941886901855\n",
      "rmse : 0.26035988330841064\n",
      "step:29 loss:6.365561485290527\n",
      "rmse : 0.24553345143795013\n",
      "step:30 loss:5.920783042907715\n",
      "rmse : 0.2313605397939682\n",
      "step:31 loss:5.566047191619873\n",
      "rmse : 0.2178776115179062\n",
      "step:32 loss:5.290340900421143\n",
      "rmse : 0.20511947572231293\n",
      "step:33 loss:5.082501411437988\n",
      "rmse : 0.1931181252002716\n",
      "step:34 loss:4.931375980377197\n",
      "rmse : 0.18190154433250427\n",
      "step:35 loss:4.826023101806641\n",
      "rmse : 0.1714925616979599\n",
      "step:36 loss:4.755918025970459\n",
      "rmse : 0.16190741956233978\n",
      "step:37 loss:4.711150646209717\n",
      "rmse : 0.15315493941307068\n",
      "step:38 loss:4.682631015777588\n",
      "rmse : 0.14523544907569885\n",
      "step:39 loss:4.662262439727783\n",
      "rmse : 0.13814061880111694\n",
      "step:40 loss:4.643088340759277\n",
      "rmse : 0.1318528801202774\n",
      "step:41 loss:4.619377136230469\n",
      "rmse : 0.12634654343128204\n",
      "step:42 loss:4.586690425872803\n",
      "rmse : 0.12158798426389694\n",
      "step:43 loss:4.5418620109558105\n",
      "rmse : 0.1175374686717987\n",
      "step:44 loss:4.482962608337402\n",
      "rmse : 0.11415029317140579\n",
      "step:45 loss:4.409185409545898\n",
      "rmse : 0.11137837171554565\n",
      "step:46 loss:4.320723056793213\n",
      "rmse : 0.10917174816131592\n",
      "step:47 loss:4.218598365783691\n",
      "rmse : 0.10747965425252914\n",
      "step:48 loss:4.104485988616943\n",
      "rmse : 0.10625118762254715\n",
      "step:49 loss:3.980527639389038\n",
      "rmse : 0.10543589293956757\n",
      "step:50 loss:3.849154472351074\n",
      "rmse : 0.10498403757810593\n",
      "step:51 loss:3.712918996810913\n",
      "rmse : 0.10484662652015686\n",
      "step:52 loss:3.574352264404297\n",
      "rmse : 0.10497535765171051\n",
      "step:53 loss:3.435835361480713\n",
      "rmse : 0.10532280057668686\n",
      "step:54 loss:3.2995119094848633\n",
      "rmse : 0.10584241151809692\n",
      "step:55 loss:3.1672165393829346\n",
      "rmse : 0.10648886859416962\n",
      "step:56 loss:3.0404369831085205\n",
      "rmse : 0.10721828788518906\n",
      "step:57 loss:2.9202921390533447\n",
      "rmse : 0.10798865556716919\n",
      "step:58 loss:2.807541847229004\n",
      "rmse : 0.1087602823972702\n",
      "step:59 loss:2.7026050090789795\n",
      "rmse : 0.1094960868358612\n",
      "step:60 loss:2.60559344291687\n",
      "rmse : 0.11016219854354858\n",
      "step:61 loss:2.5163564682006836\n",
      "rmse : 0.11072806268930435\n",
      "step:62 loss:2.4345293045043945\n",
      "rmse : 0.11116670072078705\n",
      "step:63 loss:2.3595829010009766\n",
      "rmse : 0.11145512759685516\n",
      "step:64 loss:2.290876626968384\n",
      "rmse : 0.11157418042421341\n",
      "step:65 loss:2.2277028560638428\n",
      "rmse : 0.11150866001844406\n",
      "step:66 loss:2.1693294048309326\n",
      "rmse : 0.11124730855226517\n",
      "step:67 loss:2.1150383949279785\n",
      "rmse : 0.11078283190727234\n",
      "step:68 loss:2.0641560554504395\n",
      "rmse : 0.11011155694723129\n",
      "step:69 loss:2.016072988510132\n",
      "rmse : 0.10923325270414352\n",
      "step:70 loss:1.9702619314193726\n",
      "rmse : 0.10815125703811646\n",
      "step:71 loss:1.9262884855270386\n",
      "rmse : 0.10687188059091568\n",
      "step:72 loss:1.8838145732879639\n",
      "rmse : 0.10540444403886795\n",
      "step:73 loss:1.8425966501235962\n",
      "rmse : 0.10376080125570297\n",
      "step:74 loss:1.8024789094924927\n",
      "rmse : 0.10195513069629669\n",
      "step:75 loss:1.7633858919143677\n",
      "rmse : 0.10000358521938324\n",
      "step:76 loss:1.7253073453903198\n",
      "rmse : 0.097924143075943\n",
      "step:77 loss:1.6882898807525635\n",
      "rmse : 0.09573622047901154\n",
      "step:78 loss:1.6524198055267334\n",
      "rmse : 0.09346010535955429\n",
      "step:79 loss:1.617805004119873\n",
      "rmse : 0.09111712872982025\n",
      "step:80 loss:1.5845706462860107\n",
      "rmse : 0.08872876316308975\n",
      "step:81 loss:1.552836298942566\n",
      "rmse : 0.08631666749715805\n",
      "step:82 loss:1.5227138996124268\n",
      "rmse : 0.0839020386338234\n",
      "step:83 loss:1.4942924976348877\n",
      "rmse : 0.08150557428598404\n",
      "step:84 loss:1.4676380157470703\n",
      "rmse : 0.07914678752422333\n",
      "step:85 loss:1.4427826404571533\n",
      "rmse : 0.0768439769744873\n",
      "step:86 loss:1.419730544090271\n",
      "rmse : 0.07461363077163696\n",
      "step:87 loss:1.3984514474868774\n",
      "rmse : 0.07247040420770645\n",
      "step:88 loss:1.3788870573043823\n",
      "rmse : 0.07042676210403442\n",
      "step:89 loss:1.360954999923706\n",
      "rmse : 0.0684928372502327\n",
      "step:90 loss:1.344551682472229\n",
      "rmse : 0.06667634844779968\n",
      "step:91 loss:1.3295596837997437\n",
      "rmse : 0.06498260796070099\n",
      "step:92 loss:1.315852403640747\n",
      "rmse : 0.06341447681188583\n",
      "step:93 loss:1.303301453590393\n",
      "rmse : 0.06197253614664078\n",
      "step:94 loss:1.291780710220337\n",
      "rmse : 0.06065542995929718\n",
      "step:95 loss:1.2811726331710815\n",
      "rmse : 0.05945981666445732\n",
      "step:96 loss:1.271368384361267\n",
      "rmse : 0.058380987495183945\n",
      "step:97 loss:1.2622745037078857\n",
      "rmse : 0.05741291120648384\n",
      "step:98 loss:1.253811001777649\n",
      "rmse : 0.056548621505498886\n",
      "step:99 loss:1.2459132671356201\n",
      "rmse : 0.055780477821826935\n",
      "step:100 loss:1.2385302782058716\n",
      "rmse : 0.055100493133068085\n",
      "step:101 loss:1.2316244840621948\n",
      "rmse : 0.054500512778759\n",
      "step:102 loss:1.225167989730835\n",
      "rmse : 0.05397222191095352\n",
      "step:103 loss:1.2191417217254639\n",
      "rmse : 0.053507592529058456\n",
      "step:104 loss:1.2135319709777832\n",
      "rmse : 0.05309876799583435\n",
      "step:105 loss:1.208328127861023\n",
      "rmse : 0.05273823067545891\n",
      "step:106 loss:1.2035208940505981\n",
      "rmse : 0.052418798208236694\n",
      "step:107 loss:1.199100375175476\n",
      "rmse : 0.05213376507163048\n",
      "step:108 loss:1.1950546503067017\n",
      "rmse : 0.05187685787677765\n",
      "step:109 loss:1.1913689374923706\n",
      "rmse : 0.05164231359958649\n",
      "step:110 loss:1.1880254745483398\n",
      "rmse : 0.05142485722899437\n",
      "step:111 loss:1.1850029230117798\n",
      "rmse : 0.05121980607509613\n",
      "step:112 loss:1.1822781562805176\n",
      "rmse : 0.05102304369211197\n",
      "step:113 loss:1.1798250675201416\n",
      "rmse : 0.05083104968070984\n",
      "step:114 loss:1.1776163578033447\n",
      "rmse : 0.05064084008336067\n",
      "step:115 loss:1.1756259202957153\n",
      "rmse : 0.05045011639595032\n",
      "step:116 loss:1.1738260984420776\n",
      "rmse : 0.050257086753845215\n",
      "step:117 loss:1.172191858291626\n",
      "rmse : 0.05006054788827896\n",
      "step:118 loss:1.1706993579864502\n",
      "rmse : 0.04985983669757843\n",
      "step:119 loss:1.169327974319458\n",
      "rmse : 0.04965471476316452\n",
      "step:120 loss:1.1680591106414795\n",
      "rmse : 0.04944548010826111\n",
      "step:121 loss:1.1668773889541626\n",
      "rmse : 0.04923268407583237\n",
      "step:122 loss:1.1657696962356567\n",
      "rmse : 0.04901731386780739\n",
      "step:123 loss:1.1647260189056396\n",
      "rmse : 0.0488005131483078\n",
      "step:124 loss:1.1637377738952637\n",
      "rmse : 0.048583630472421646\n",
      "step:125 loss:1.1627992391586304\n",
      "rmse : 0.048368096351623535\n",
      "step:126 loss:1.16190505027771\n",
      "rmse : 0.048155415803194046\n",
      "step:127 loss:1.1610515117645264\n",
      "rmse : 0.04794708266854286\n",
      "step:128 loss:1.16023588180542\n",
      "rmse : 0.047744520008563995\n",
      "step:129 loss:1.1594547033309937\n",
      "rmse : 0.04754903167486191\n",
      "step:130 loss:1.1587059497833252\n",
      "rmse : 0.0473618358373642\n",
      "step:131 loss:1.1579866409301758\n",
      "rmse : 0.04718389734625816\n",
      "step:132 loss:1.1572940349578857\n",
      "rmse : 0.04701611027121544\n",
      "step:133 loss:1.1566247940063477\n",
      "rmse : 0.04685908555984497\n",
      "step:134 loss:1.15597665309906\n",
      "rmse : 0.046713292598724365\n",
      "step:135 loss:1.155346155166626\n",
      "rmse : 0.04657907411456108\n",
      "step:136 loss:1.154729962348938\n",
      "rmse : 0.04645650088787079\n",
      "step:137 loss:1.1541249752044678\n",
      "rmse : 0.046345535665750504\n",
      "step:138 loss:1.153529405593872\n",
      "rmse : 0.04624594748020172\n",
      "step:139 loss:1.1529394388198853\n",
      "rmse : 0.04615745320916176\n",
      "step:140 loss:1.1523536443710327\n",
      "rmse : 0.0460795983672142\n",
      "step:141 loss:1.1517701148986816\n",
      "rmse : 0.0460117943584919\n",
      "step:142 loss:1.1511874198913574\n",
      "rmse : 0.04595346748828888\n",
      "step:143 loss:1.1506047248840332\n",
      "rmse : 0.04590389132499695\n",
      "step:144 loss:1.1500216722488403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse : 0.045862335711717606\n",
      "step:145 loss:1.1494375467300415\n",
      "rmse : 0.045828018337488174\n",
      "step:146 loss:1.1488522291183472\n",
      "rmse : 0.04580013081431389\n",
      "step:147 loss:1.1482656002044678\n",
      "rmse : 0.045777853578329086\n",
      "step:148 loss:1.1476783752441406\n",
      "rmse : 0.045760393142700195\n",
      "step:149 loss:1.1470903158187866\n",
      "rmse : 0.04574691876769066\n",
      "step:150 loss:1.146501898765564\n",
      "rmse : 0.04573671892285347\n",
      "step:151 loss:1.145913004875183\n",
      "rmse : 0.04572901874780655\n",
      "step:152 loss:1.1453245878219604\n",
      "rmse : 0.045723140239715576\n",
      "step:153 loss:1.1447360515594482\n",
      "rmse : 0.0457184761762619\n",
      "step:154 loss:1.1441473960876465\n",
      "rmse : 0.04571443051099777\n",
      "step:155 loss:1.143559217453003\n",
      "rmse : 0.04571055248379707\n",
      "step:156 loss:1.1429708003997803\n",
      "rmse : 0.04570633918046951\n",
      "step:157 loss:1.142382025718689\n",
      "rmse : 0.045701511204242706\n",
      "step:158 loss:1.1417937278747559\n",
      "rmse : 0.04569576680660248\n",
      "step:159 loss:1.1412049531936646\n",
      "rmse : 0.04568890854716301\n",
      "step:160 loss:1.1406159400939941\n",
      "rmse : 0.04568079113960266\n",
      "step:161 loss:1.1400268077850342\n",
      "rmse : 0.045671336352825165\n",
      "step:162 loss:1.139437198638916\n",
      "rmse : 0.04566054418683052\n",
      "step:163 loss:1.1388471126556396\n",
      "rmse : 0.04564844071865082\n",
      "step:164 loss:1.1382572650909424\n",
      "rmse : 0.04563508927822113\n",
      "step:165 loss:1.1376668214797974\n",
      "rmse : 0.04562067240476608\n",
      "step:166 loss:1.137076497077942\n",
      "rmse : 0.045605264604091644\n",
      "step:167 loss:1.136486291885376\n",
      "rmse : 0.04558907076716423\n",
      "step:168 loss:1.135895848274231\n",
      "rmse : 0.04557223618030548\n",
      "step:169 loss:1.1353058815002441\n",
      "rmse : 0.045555006712675095\n",
      "step:170 loss:1.134716272354126\n",
      "rmse : 0.04553750529885292\n",
      "step:171 loss:1.1341266632080078\n",
      "rmse : 0.04551994800567627\n",
      "step:172 loss:1.1335376501083374\n",
      "rmse : 0.04550248384475708\n",
      "step:173 loss:1.1329492330551147\n",
      "rmse : 0.0454852432012558\n",
      "step:174 loss:1.1323611736297607\n",
      "rmse : 0.04546837508678436\n",
      "step:175 loss:1.131773591041565\n",
      "rmse : 0.04545198753476143\n",
      "step:176 loss:1.1311864852905273\n",
      "rmse : 0.045436132699251175\n",
      "step:177 loss:1.1306002140045166\n",
      "rmse : 0.045420870184898376\n",
      "step:178 loss:1.1300138235092163\n",
      "rmse : 0.045406267046928406\n",
      "step:179 loss:1.1294282674789429\n",
      "rmse : 0.045392319560050964\n",
      "step:180 loss:1.1288431882858276\n",
      "rmse : 0.04537900164723396\n",
      "step:181 loss:1.128258466720581\n",
      "rmse : 0.04536627233028412\n",
      "step:182 loss:1.1276741027832031\n",
      "rmse : 0.04535409063100815\n",
      "step:183 loss:1.1270900964736938\n",
      "rmse : 0.04534241929650307\n",
      "step:184 loss:1.1265065670013428\n",
      "rmse : 0.045331139117479324\n",
      "step:185 loss:1.1259236335754395\n",
      "rmse : 0.045320186764001846\n",
      "step:186 loss:1.1253411769866943\n",
      "rmse : 0.045309484004974365\n",
      "step:187 loss:1.1247589588165283\n",
      "rmse : 0.04529890790581703\n",
      "step:188 loss:1.124177098274231\n",
      "rmse : 0.04528835415840149\n",
      "step:189 loss:1.1235960721969604\n",
      "rmse : 0.04527777060866356\n",
      "step:190 loss:1.1230151653289795\n",
      "rmse : 0.045267071574926376\n",
      "step:191 loss:1.1224350929260254\n",
      "rmse : 0.045256149023771286\n",
      "step:192 loss:1.1218550205230713\n",
      "rmse : 0.045244935899972916\n",
      "step:193 loss:1.121275782585144\n",
      "rmse : 0.04523336887359619\n",
      "step:194 loss:1.1206971406936646\n",
      "rmse : 0.04522142931818962\n",
      "step:195 loss:1.1201187372207642\n",
      "rmse : 0.04520900920033455\n",
      "step:196 loss:1.119540810585022\n",
      "rmse : 0.04519612342119217\n",
      "step:197 loss:1.118963599205017\n",
      "rmse : 0.0451827310025692\n",
      "step:198 loss:1.1183867454528809\n",
      "rmse : 0.045168861746788025\n",
      "step:199 loss:1.1178102493286133\n",
      "rmse : 0.045154470950365067\n",
      "step:200 loss:1.117234468460083\n",
      "rmse : 0.045139625668525696\n",
      "step:201 loss:1.1166590452194214\n",
      "rmse : 0.045124296098947525\n",
      "step:202 loss:1.116084098815918\n",
      "rmse : 0.04510853812098503\n",
      "step:203 loss:1.1155096292495728\n",
      "rmse : 0.04509241506457329\n",
      "step:204 loss:1.1149356365203857\n",
      "rmse : 0.0450759083032608\n",
      "step:205 loss:1.1143620014190674\n",
      "rmse : 0.04505909979343414\n",
      "step:206 loss:1.1137890815734863\n",
      "rmse : 0.04504202678799629\n",
      "step:207 loss:1.113216519355774\n",
      "rmse : 0.045024726539850235\n",
      "step:208 loss:1.1126445531845093\n",
      "rmse : 0.04500725492835045\n",
      "step:209 loss:1.1120731830596924\n",
      "rmse : 0.04498964175581932\n",
      "step:210 loss:1.1115021705627441\n",
      "rmse : 0.044971950352191925\n",
      "step:211 loss:1.110931634902954\n",
      "rmse : 0.04495418816804886\n",
      "step:212 loss:1.1103616952896118\n",
      "rmse : 0.044936418533325195\n",
      "step:213 loss:1.1097924709320068\n",
      "rmse : 0.04491864889860153\n",
      "step:214 loss:1.109223484992981\n",
      "rmse : 0.04490092024207115\n",
      "step:215 loss:1.108655333518982\n",
      "rmse : 0.04488325119018555\n",
      "step:216 loss:1.1080875396728516\n",
      "rmse : 0.04486563429236412\n",
      "step:217 loss:1.1075201034545898\n",
      "rmse : 0.044848088175058365\n",
      "step:218 loss:1.106953501701355\n",
      "rmse : 0.04483065381646156\n",
      "step:219 loss:1.1063873767852783\n",
      "rmse : 0.044813286513090134\n",
      "step:220 loss:1.1058217287063599\n",
      "rmse : 0.04479599744081497\n",
      "step:221 loss:1.1052567958831787\n",
      "rmse : 0.044778794050216675\n",
      "step:222 loss:1.1046924591064453\n",
      "rmse : 0.04476168006658554\n",
      "step:223 loss:1.1041285991668701\n",
      "rmse : 0.04474461078643799\n",
      "step:224 loss:1.1035652160644531\n",
      "rmse : 0.04472761228680611\n",
      "step:225 loss:1.1030025482177734\n",
      "rmse : 0.04471064731478691\n",
      "step:226 loss:1.1024401187896729\n",
      "rmse : 0.0446937121450901\n",
      "step:227 loss:1.1018787622451782\n",
      "rmse : 0.044676825404167175\n",
      "step:228 loss:1.1013177633285522\n",
      "rmse : 0.044659946113824844\n",
      "step:229 loss:1.100757122039795\n",
      "rmse : 0.04464306682348251\n",
      "step:230 loss:1.100197434425354\n",
      "rmse : 0.04462617635726929\n",
      "step:231 loss:1.0996382236480713\n",
      "rmse : 0.04460930451750755\n",
      "step:232 loss:1.0990794897079468\n",
      "rmse : 0.04459239915013313\n",
      "step:233 loss:1.0985215902328491\n",
      "rmse : 0.04457547143101692\n",
      "step:234 loss:1.0979639291763306\n",
      "rmse : 0.044558554887771606\n",
      "step:235 loss:1.0974071025848389\n",
      "rmse : 0.044541601091623306\n",
      "step:236 loss:1.0968507528305054\n",
      "rmse : 0.04452461749315262\n",
      "step:237 loss:1.0962951183319092\n",
      "rmse : 0.04450765997171402\n",
      "step:238 loss:1.0957403182983398\n",
      "rmse : 0.04449065029621124\n",
      "step:239 loss:1.0951857566833496\n",
      "rmse : 0.04447367042303085\n",
      "step:240 loss:1.0946319103240967\n",
      "rmse : 0.044456686824560165\n",
      "step:241 loss:1.0940786600112915\n",
      "rmse : 0.04443969950079918\n",
      "step:242 loss:1.0935261249542236\n",
      "rmse : 0.04442271962761879\n",
      "step:243 loss:1.092974066734314\n",
      "rmse : 0.04440578073263168\n",
      "step:244 loss:1.0924228429794312\n",
      "rmse : 0.04438890144228935\n",
      "step:245 loss:1.091872215270996\n",
      "rmse : 0.04437201842665672\n",
      "step:246 loss:1.0913217067718506\n",
      "rmse : 0.044355206191539764\n",
      "step:247 loss:1.0907725095748901\n",
      "rmse : 0.04433842748403549\n",
      "step:248 loss:1.0902235507965088\n",
      "rmse : 0.044321704655885696\n",
      "step:249 loss:1.0896754264831543\n",
      "rmse : 0.044305045157670975\n",
      "step:250 loss:1.089127540588379\n",
      "rmse : 0.04428846761584282\n",
      "step:251 loss:1.0885807275772095\n",
      "rmse : 0.04427193105220795\n",
      "step:252 loss:1.0880345106124878\n",
      "rmse : 0.04425547271966934\n",
      "step:253 loss:1.0874887704849243\n",
      "rmse : 0.04423908889293671\n",
      "step:254 loss:1.086943507194519\n",
      "rmse : 0.044222764670848846\n",
      "step:255 loss:1.0863991975784302\n",
      "rmse : 0.044206488877534866\n",
      "step:256 loss:1.08585524559021\n",
      "rmse : 0.04419030621647835\n",
      "step:257 loss:1.085312008857727\n",
      "rmse : 0.0441741906106472\n",
      "step:258 loss:1.084769368171692\n",
      "rmse : 0.044158123433589935\n",
      "step:259 loss:1.0842275619506836\n",
      "rmse : 0.04414213448762894\n",
      "step:260 loss:1.083686351776123\n",
      "rmse : 0.04412618651986122\n",
      "step:261 loss:1.0831458568572998\n",
      "rmse : 0.04411027953028679\n",
      "step:262 loss:1.0826056003570557\n",
      "rmse : 0.044094424694776535\n",
      "step:263 loss:1.0820664167404175\n",
      "rmse : 0.044078610837459564\n",
      "step:264 loss:1.081527829170227\n",
      "rmse : 0.04406286031007767\n",
      "step:265 loss:1.0809894800186157\n",
      "rmse : 0.044047147035598755\n",
      "step:266 loss:1.0804522037506104\n",
      "rmse : 0.044031452387571335\n",
      "step:267 loss:1.0799154043197632\n",
      "rmse : 0.04401580989360809\n",
      "step:268 loss:1.0793793201446533\n",
      "rmse : 0.04400022327899933\n",
      "step:269 loss:1.0788438320159912\n",
      "rmse : 0.04398464784026146\n",
      "step:270 loss:1.0783089399337769\n",
      "rmse : 0.04396909102797508\n",
      "step:271 loss:1.0777747631072998\n",
      "rmse : 0.043953604996204376\n",
      "step:272 loss:1.077241063117981\n",
      "rmse : 0.04393815994262695\n",
      "step:273 loss:1.0767083168029785\n",
      "rmse : 0.04392270743846893\n",
      "step:274 loss:1.0761760473251343\n",
      "rmse : 0.043907295912504196\n",
      "step:275 loss:1.0756442546844482\n",
      "rmse : 0.04389192536473274\n",
      "step:276 loss:1.0751134157180786\n",
      "rmse : 0.04387657716870308\n",
      "step:277 loss:1.074582815170288\n",
      "rmse : 0.0438612662255764\n",
      "step:278 loss:1.0740530490875244\n",
      "rmse : 0.043845996260643005\n",
      "step:279 loss:1.073523998260498\n",
      "rmse : 0.043830763548612595\n",
      "step:280 loss:1.072995662689209\n",
      "rmse : 0.04381556808948517\n",
      "step:281 loss:1.0724676847457886\n",
      "rmse : 0.04380042850971222\n",
      "step:282 loss:1.0719406604766846\n",
      "rmse : 0.04378531128168106\n",
      "step:283 loss:1.0714141130447388\n",
      "rmse : 0.04377023130655289\n",
      "step:284 loss:1.0708882808685303\n",
      "rmse : 0.043755173683166504\n",
      "step:285 loss:1.0703630447387695\n",
      "rmse : 0.0437401719391346\n",
      "step:286 loss:1.069838523864746\n",
      "rmse : 0.04372520372271538\n",
      "step:287 loss:1.0693144798278809\n",
      "rmse : 0.04371028020977974\n",
      "step:288 loss:1.068791151046753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse : 0.04369538277387619\n",
      "step:289 loss:1.0682686567306519\n",
      "rmse : 0.04368051886558533\n",
      "step:290 loss:1.0677465200424194\n",
      "rmse : 0.043665725737810135\n",
      "step:291 loss:1.0672252178192139\n",
      "rmse : 0.04365096613764763\n",
      "step:292 loss:1.066704273223877\n",
      "rmse : 0.043636221438646317\n",
      "step:293 loss:1.066184163093567\n",
      "rmse : 0.04362152889370918\n",
      "step:294 loss:1.065664529800415\n",
      "rmse : 0.043606869876384735\n",
      "step:295 loss:1.0651458501815796\n",
      "rmse : 0.043592218309640884\n",
      "step:296 loss:1.0646274089813232\n",
      "rmse : 0.04357762634754181\n",
      "step:297 loss:1.0641101598739624\n",
      "rmse : 0.04356304556131363\n",
      "step:298 loss:1.0635930299758911\n",
      "rmse : 0.04354850947856903\n",
      "step:299 loss:1.0630767345428467\n",
      "rmse : 0.04353402554988861\n",
      "step:300 loss:1.06256103515625\n",
      "rmse : 0.04351956397294998\n",
      "step:301 loss:1.0620461702346802\n",
      "rmse : 0.04350513964891434\n",
      "step:302 loss:1.0615315437316895\n",
      "rmse : 0.04349072650074959\n",
      "step:303 loss:1.061017632484436\n",
      "rmse : 0.04347635805606842\n",
      "step:304 loss:1.06050443649292\n",
      "rmse : 0.04346199706196785\n",
      "step:305 loss:1.0599919557571411\n",
      "rmse : 0.04344768449664116\n",
      "step:306 loss:1.059479832649231\n",
      "rmse : 0.04343339428305626\n",
      "step:307 loss:1.0589686632156372\n",
      "rmse : 0.043419141322374344\n",
      "step:308 loss:1.0584580898284912\n",
      "rmse : 0.04340492561459541\n",
      "step:309 loss:1.0579478740692139\n",
      "rmse : 0.04339073970913887\n",
      "step:310 loss:1.057438611984253\n",
      "rmse : 0.043376561254262924\n",
      "step:311 loss:1.0569297075271606\n",
      "rmse : 0.04336243122816086\n",
      "step:312 loss:1.0564213991165161\n",
      "rmse : 0.04334833100438118\n",
      "step:313 loss:1.0559138059616089\n",
      "rmse : 0.0433342382311821\n",
      "step:314 loss:1.0554066896438599\n",
      "rmse : 0.04332020878791809\n",
      "step:315 loss:1.0549004077911377\n",
      "rmse : 0.04330619052052498\n",
      "step:316 loss:1.0543943643569946\n",
      "rmse : 0.04329220950603485\n",
      "step:317 loss:1.053889513015747\n",
      "rmse : 0.04327823221683502\n",
      "step:318 loss:1.0533849000930786\n",
      "rmse : 0.043264321982860565\n",
      "step:319 loss:1.052881121635437\n",
      "rmse : 0.04325040802359581\n",
      "step:320 loss:1.052377462387085\n",
      "rmse : 0.04323654621839523\n",
      "step:321 loss:1.0518748760223389\n",
      "rmse : 0.04322269931435585\n",
      "step:322 loss:1.0513725280761719\n",
      "rmse : 0.04320887103676796\n",
      "step:323 loss:1.0508711338043213\n",
      "rmse : 0.043195098638534546\n",
      "step:324 loss:1.050370216369629\n",
      "rmse : 0.04318135231733322\n",
      "step:325 loss:1.0498698949813843\n",
      "rmse : 0.04316761717200279\n",
      "step:326 loss:1.049370288848877\n",
      "rmse : 0.04315391555428505\n",
      "step:327 loss:1.0488712787628174\n",
      "rmse : 0.0431402362883091\n",
      "step:328 loss:1.048372745513916\n",
      "rmse : 0.04312660172581673\n",
      "step:329 loss:1.0478746891021729\n",
      "rmse : 0.04311300069093704\n",
      "step:330 loss:1.0473774671554565\n",
      "rmse : 0.04309942200779915\n",
      "step:331 loss:1.0468806028366089\n",
      "rmse : 0.04308587685227394\n",
      "step:332 loss:1.0463844537734985\n",
      "rmse : 0.04307233542203903\n",
      "step:333 loss:1.0458887815475464\n",
      "rmse : 0.043058838695287704\n",
      "step:334 loss:1.0453940629959106\n",
      "rmse : 0.04304537549614906\n",
      "step:335 loss:1.044899582862854\n",
      "rmse : 0.043031930923461914\n",
      "step:336 loss:1.0444055795669556\n",
      "rmse : 0.04301851615309715\n",
      "step:337 loss:1.043912649154663\n",
      "rmse : 0.043005116283893585\n",
      "step:338 loss:1.0434198379516602\n",
      "rmse : 0.042991749942302704\n",
      "step:339 loss:1.0429277420043945\n",
      "rmse : 0.04297841340303421\n",
      "step:340 loss:1.0424362421035767\n",
      "rmse : 0.042965102940797806\n",
      "step:341 loss:1.041945457458496\n",
      "rmse : 0.042951811105012894\n",
      "step:342 loss:1.0414551496505737\n",
      "rmse : 0.04293857514858246\n",
      "step:343 loss:1.0409653186798096\n",
      "rmse : 0.04292531684041023\n",
      "step:344 loss:1.0404760837554932\n",
      "rmse : 0.04291212186217308\n",
      "step:345 loss:1.0399874448776245\n",
      "rmse : 0.04289894551038742\n",
      "step:346 loss:1.0394994020462036\n",
      "rmse : 0.042885784059762955\n",
      "step:347 loss:1.0390119552612305\n",
      "rmse : 0.04287265986204147\n",
      "step:348 loss:1.038524866104126\n",
      "rmse : 0.04285954311490059\n",
      "step:349 loss:1.0380386114120483\n",
      "rmse : 0.042846452444791794\n",
      "step:350 loss:1.0375527143478394\n",
      "rmse : 0.04283338040113449\n",
      "step:351 loss:1.0370674133300781\n",
      "rmse : 0.042820364236831665\n",
      "step:352 loss:1.0365829467773438\n",
      "rmse : 0.04280735179781914\n",
      "step:353 loss:1.036098599433899\n",
      "rmse : 0.04279434680938721\n",
      "step:354 loss:1.0356149673461914\n",
      "rmse : 0.042781390249729156\n",
      "step:355 loss:1.0351320505142212\n",
      "rmse : 0.042768444865942\n",
      "step:356 loss:1.0346494913101196\n",
      "rmse : 0.04275553673505783\n",
      "step:357 loss:1.034167766571045\n",
      "rmse : 0.04274264723062515\n",
      "step:358 loss:1.0336863994598389\n",
      "rmse : 0.04272976890206337\n",
      "step:359 loss:1.0332053899765015\n",
      "rmse : 0.04271692410111427\n",
      "step:360 loss:1.032725214958191\n",
      "rmse : 0.04270411655306816\n",
      "step:361 loss:1.032245397567749\n",
      "rmse : 0.042691294103860855\n",
      "step:362 loss:1.0317661762237549\n",
      "rmse : 0.042678508907556534\n",
      "step:363 loss:1.031287431716919\n",
      "rmse : 0.0426657535135746\n",
      "step:364 loss:1.0308094024658203\n",
      "rmse : 0.04265303164720535\n",
      "step:365 loss:1.0303318500518799\n",
      "rmse : 0.0426403284072876\n",
      "step:366 loss:1.029854655265808\n",
      "rmse : 0.04262762516736984\n",
      "step:367 loss:1.0293781757354736\n",
      "rmse : 0.042614955455064774\n",
      "step:368 loss:1.0289021730422974\n",
      "rmse : 0.042602334171533585\n",
      "step:369 loss:1.0284266471862793\n",
      "rmse : 0.0425897017121315\n",
      "step:370 loss:1.0279515981674194\n",
      "rmse : 0.0425771102309227\n",
      "step:371 loss:1.0274773836135864\n",
      "rmse : 0.0425645187497139\n",
      "step:372 loss:1.0270034074783325\n",
      "rmse : 0.042551975697278976\n",
      "step:373 loss:1.0265299081802368\n",
      "rmse : 0.04253944009542465\n",
      "step:374 loss:1.026057243347168\n",
      "rmse : 0.04252691566944122\n",
      "step:375 loss:1.0255848169326782\n",
      "rmse : 0.042514435946941376\n",
      "step:376 loss:1.0251129865646362\n",
      "rmse : 0.042501941323280334\n",
      "step:377 loss:1.0246416330337524\n",
      "rmse : 0.04248950630426407\n",
      "step:378 loss:1.0241708755493164\n",
      "rmse : 0.0424770787358284\n",
      "step:379 loss:1.0237005949020386\n",
      "rmse : 0.04246465861797333\n",
      "step:380 loss:1.023230791091919\n",
      "rmse : 0.042452264577150345\n",
      "step:381 loss:1.022761583328247\n",
      "rmse : 0.04243987426161766\n",
      "step:382 loss:1.0222927331924438\n",
      "rmse : 0.042427536100149155\n",
      "step:383 loss:1.0218243598937988\n",
      "rmse : 0.04241516441106796\n",
      "step:384 loss:1.0213565826416016\n",
      "rmse : 0.04240286350250244\n",
      "step:385 loss:1.0208895206451416\n",
      "rmse : 0.04239053651690483\n",
      "step:386 loss:1.0204229354858398\n",
      "rmse : 0.042378246784210205\n",
      "step:387 loss:1.0199564695358276\n",
      "rmse : 0.04236597195267677\n",
      "step:388 loss:1.0194907188415527\n",
      "rmse : 0.04235371947288513\n",
      "step:389 loss:1.0190253257751465\n",
      "rmse : 0.04234149307012558\n",
      "step:390 loss:1.018560528755188\n",
      "rmse : 0.04232925921678543\n",
      "step:391 loss:1.0180962085723877\n",
      "rmse : 0.04231706261634827\n",
      "step:392 loss:1.0176324844360352\n",
      "rmse : 0.042304910719394684\n",
      "step:393 loss:1.0171691179275513\n",
      "rmse : 0.042292725294828415\n",
      "step:394 loss:1.0167062282562256\n",
      "rmse : 0.042280588299036026\n",
      "step:395 loss:1.0162439346313477\n",
      "rmse : 0.04226848483085632\n",
      "step:396 loss:1.0157819986343384\n",
      "rmse : 0.04225638508796692\n",
      "step:397 loss:1.0153206586837769\n",
      "rmse : 0.04224429279565811\n",
      "step:398 loss:1.0148597955703735\n",
      "rmse : 0.0422322042286396\n",
      "step:399 loss:1.0143992900848389\n",
      "rmse : 0.04222014173865318\n",
      "step:400 loss:1.013939380645752\n",
      "rmse : 0.04220810532569885\n",
      "step:401 loss:1.0134798288345337\n",
      "rmse : 0.04219606891274452\n",
      "step:402 loss:1.0130207538604736\n",
      "rmse : 0.042184069752693176\n",
      "step:403 loss:1.0125623941421509\n",
      "rmse : 0.042172063142061234\n",
      "step:404 loss:1.0121040344238281\n",
      "rmse : 0.04216007888317108\n",
      "step:405 loss:1.0116465091705322\n",
      "rmse : 0.042148128151893616\n",
      "step:406 loss:1.011189341545105\n",
      "rmse : 0.04213618114590645\n",
      "step:407 loss:1.010732650756836\n",
      "rmse : 0.04212424159049988\n",
      "step:408 loss:1.010276436805725\n",
      "rmse : 0.0421123132109642\n",
      "step:409 loss:1.0098206996917725\n",
      "rmse : 0.04210040718317032\n",
      "step:410 loss:1.009365200996399\n",
      "rmse : 0.04208850488066673\n",
      "step:411 loss:1.0089101791381836\n",
      "rmse : 0.04207662492990494\n",
      "step:412 loss:1.0084559917449951\n",
      "rmse : 0.04206477850675583\n",
      "step:413 loss:1.0080019235610962\n",
      "rmse : 0.04205290973186493\n",
      "step:414 loss:1.0075483322143555\n",
      "rmse : 0.04204108193516731\n",
      "step:415 loss:1.007095456123352\n",
      "rmse : 0.042029254138469696\n",
      "step:416 loss:1.0066425800323486\n",
      "rmse : 0.042017459869384766\n",
      "step:417 loss:1.006190538406372\n",
      "rmse : 0.04200565069913864\n",
      "step:418 loss:1.0057388544082642\n",
      "rmse : 0.04199385643005371\n",
      "step:419 loss:1.005287528038025\n",
      "rmse : 0.041982099413871765\n",
      "step:420 loss:1.0048365592956543\n",
      "rmse : 0.04197033867239952\n",
      "step:421 loss:1.0043861865997314\n",
      "rmse : 0.04195859283208847\n",
      "step:422 loss:1.0039361715316772\n",
      "rmse : 0.041946861892938614\n",
      "step:423 loss:1.0034866333007812\n",
      "rmse : 0.041935160756111145\n",
      "step:424 loss:1.0030375719070435\n",
      "rmse : 0.04192344471812248\n",
      "step:425 loss:1.0025888681411743\n",
      "rmse : 0.04191175103187561\n",
      "step:426 loss:1.0021405220031738\n",
      "rmse : 0.04190007224678993\n",
      "step:427 loss:1.0016928911209106\n",
      "rmse : 0.041888412088155746\n",
      "step:428 loss:1.001245379447937\n",
      "rmse : 0.041876763105392456\n",
      "step:429 loss:1.0007983446121216\n",
      "rmse : 0.04186512157320976\n",
      "step:430 loss:1.000351905822754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse : 0.041853468865156174\n",
      "step:431 loss:0.9999057650566101\n",
      "rmse : 0.04184185341000557\n",
      "step:432 loss:0.9994601011276245\n",
      "rmse : 0.04183021932840347\n",
      "step:433 loss:0.9990146160125732\n",
      "rmse : 0.04181862622499466\n",
      "step:434 loss:0.9985697269439697\n",
      "rmse : 0.04180704057216644\n",
      "step:435 loss:0.9981253147125244\n",
      "rmse : 0.04179546236991882\n",
      "step:436 loss:0.997681200504303\n",
      "rmse : 0.041783902794122696\n",
      "step:437 loss:0.997237503528595\n",
      "rmse : 0.04177232086658478\n",
      "step:438 loss:0.9967943429946899\n",
      "rmse : 0.04176078364253044\n",
      "step:439 loss:0.9963513612747192\n",
      "rmse : 0.0417492613196373\n",
      "step:440 loss:0.9959088563919067\n",
      "rmse : 0.04173772409558296\n",
      "step:441 loss:0.9954670071601868\n",
      "rmse : 0.04172620549798012\n",
      "step:442 loss:0.9950253963470459\n",
      "rmse : 0.04171469807624817\n",
      "step:443 loss:0.9945842027664185\n",
      "rmse : 0.041703201830387115\n",
      "step:444 loss:0.9941433668136597\n",
      "rmse : 0.041691701859235764\n",
      "step:445 loss:0.9937030076980591\n",
      "rmse : 0.041680216789245605\n",
      "step:446 loss:0.9932630658149719\n",
      "rmse : 0.04166875034570694\n",
      "step:447 loss:0.9928233027458191\n",
      "rmse : 0.04165727272629738\n",
      "step:448 loss:0.9923840165138245\n",
      "rmse : 0.04164582118391991\n",
      "step:449 loss:0.9919452667236328\n",
      "rmse : 0.04163438081741333\n",
      "step:450 loss:0.9915067553520203\n",
      "rmse : 0.041622936725616455\n",
      "step:451 loss:0.9910686612129211\n",
      "rmse : 0.041611503809690475\n",
      "step:452 loss:0.9906312823295593\n",
      "rmse : 0.041600074619054794\n",
      "step:453 loss:0.9901938438415527\n",
      "rmse : 0.041588664054870605\n",
      "step:454 loss:0.9897570610046387\n",
      "rmse : 0.041577260941267014\n",
      "step:455 loss:0.9893205165863037\n",
      "rmse : 0.04156586527824402\n",
      "step:456 loss:0.9888843297958374\n",
      "rmse : 0.041554469615221024\n",
      "step:457 loss:0.9884485602378845\n",
      "rmse : 0.041543081402778625\n",
      "step:458 loss:0.9880132079124451\n",
      "rmse : 0.041531700640916824\n",
      "step:459 loss:0.987578272819519\n",
      "rmse : 0.041520338505506516\n",
      "step:460 loss:0.9871436953544617\n",
      "rmse : 0.041508980095386505\n",
      "step:461 loss:0.986709475517273\n",
      "rmse : 0.041497621685266495\n",
      "step:462 loss:0.9862754940986633\n",
      "rmse : 0.04148629680275917\n",
      "step:463 loss:0.9858419299125671\n",
      "rmse : 0.041474949568510056\n",
      "step:464 loss:0.9854089617729187\n",
      "rmse : 0.041463617235422134\n",
      "step:465 loss:0.9849762916564941\n",
      "rmse : 0.04145228862762451\n",
      "step:466 loss:0.9845439791679382\n",
      "rmse : 0.041440967470407486\n",
      "step:467 loss:0.9841119050979614\n",
      "rmse : 0.041429657489061356\n",
      "step:468 loss:0.983680248260498\n",
      "rmse : 0.041418321430683136\n",
      "step:469 loss:0.9832489490509033\n",
      "rmse : 0.0414070263504982\n",
      "step:470 loss:0.9828178882598877\n",
      "rmse : 0.04139576107263565\n",
      "step:471 loss:0.9823874831199646\n",
      "rmse : 0.041384462267160416\n",
      "step:472 loss:0.9819573163986206\n",
      "rmse : 0.04137318208813667\n",
      "step:473 loss:0.9815274477005005\n",
      "rmse : 0.041361890733242035\n",
      "step:474 loss:0.981097936630249\n",
      "rmse : 0.041350606828927994\n",
      "step:475 loss:0.9806689023971558\n",
      "rmse : 0.04133934900164604\n",
      "step:476 loss:0.980239987373352\n",
      "rmse : 0.0413280725479126\n",
      "step:477 loss:0.9798116683959961\n",
      "rmse : 0.04131680727005005\n",
      "step:478 loss:0.979383647441864\n",
      "rmse : 0.0413055494427681\n",
      "step:479 loss:0.978955864906311\n",
      "rmse : 0.041294317692518234\n",
      "step:480 loss:0.978528618812561\n",
      "rmse : 0.041283078491687775\n",
      "step:481 loss:0.9781015515327454\n",
      "rmse : 0.04127182811498642\n",
      "step:482 loss:0.9776748418807983\n",
      "rmse : 0.04126059263944626\n",
      "step:483 loss:0.9772483706474304\n",
      "rmse : 0.04124937579035759\n",
      "step:484 loss:0.9768224954605103\n",
      "rmse : 0.041238147765398026\n",
      "step:485 loss:0.9763967394828796\n",
      "rmse : 0.04122691601514816\n",
      "step:486 loss:0.975971519947052\n",
      "rmse : 0.0412156879901886\n",
      "step:487 loss:0.975546658039093\n",
      "rmse : 0.04120447859168053\n",
      "step:488 loss:0.9751219153404236\n",
      "rmse : 0.041193269193172455\n",
      "step:489 loss:0.9746975302696228\n",
      "rmse : 0.041182052344083786\n",
      "step:490 loss:0.9742735624313354\n",
      "rmse : 0.04117085412144661\n",
      "step:491 loss:0.9738500714302063\n",
      "rmse : 0.04115966334939003\n",
      "step:492 loss:0.9734269976615906\n",
      "rmse : 0.04114845395088196\n",
      "step:493 loss:0.9730039238929749\n",
      "rmse : 0.04113728180527687\n",
      "step:494 loss:0.9725812673568726\n",
      "rmse : 0.04112608730792999\n",
      "step:495 loss:0.9721589088439941\n",
      "rmse : 0.04111489653587341\n",
      "step:496 loss:0.9717370867729187\n",
      "rmse : 0.04110369831323624\n",
      "step:497 loss:0.971315324306488\n",
      "rmse : 0.041092537343502045\n",
      "step:498 loss:0.9708940386772156\n",
      "rmse : 0.04108133167028427\n",
      "step:499 loss:0.9704731702804565\n",
      "rmse : 0.041070159524679184\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(dataY)*0.7)\n",
    "test_size = len(dataY)-train_size\n",
    "trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:len(dataX)])\n",
    "trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:len(dataY)])\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, seq_len, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:,-1], output_dim, activation_fn=None)\n",
    "\n",
    "loss = tf.reduce_sum(tf.square(Y_pred-Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets-predictions)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(iter):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X:trainX, Y:trainY})\n",
    "        print(\"step:{} loss:{}\".format(i, step_loss))\n",
    "        \n",
    "        test_predict = sess.run(Y_pred, feed_dict={X:testX})\n",
    "        rmse_val = sess.run(rmse, feed_dict={targets:testY, predictions:test_predict})\n",
    "        print(\"rmse : {}\".format(rmse_val))\n",
    "        \n",
    "        plt.plot(testY)\n",
    "        plt.plot(test_predict)\n",
    "        plt.xlabel(\"Time\"); plt.ylabel(\"stock price\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
